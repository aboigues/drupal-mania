# Configuration Fluentd pour la pile EFK
# Mode asynchrone pour optimiser les performances

# Source: Collecte des logs Docker
<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>

# Source: Logs du système Fluentd lui-même
<source>
  @type monitor_agent
  bind 0.0.0.0
  port 24220
</source>

# Filtre: Parsing des logs JSON si applicable
<filter docker.**>
  @type parser
  key_name log
  reserve_data true
  <parse>
    @type json
  </parse>
</filter>

# Filtre: Ajout de métadonnées
<filter docker.**>
  @type record_transformer
  <record>
    hostname "#{Socket.gethostname}"
    tag ${tag}
    time ${time}
  </record>
</filter>

# Sortie: Elasticsearch avec buffers asynchrones
<match docker.**>
  @type elasticsearch
  host elasticsearch
  port 9200
  logstash_format true
  logstash_prefix drupal-logs
  logstash_dateformat %Y.%m.%d
  include_tag_key true
  tag_key @log_name

  # Configuration du buffer en mode ASYNCHRONE
  <buffer>
    @type file
    path /fluentd/log/buffer/docker
    flush_mode async
    flush_thread_count 4
    flush_interval 5s
    flush_at_shutdown true
    retry_type exponential_backoff
    retry_wait 1s
    retry_max_interval 60s
    retry_timeout 1h
    overflow_action block
    chunk_limit_size 5M
    queue_limit_length 32
    compress gzip
  </buffer>

  # Gestion des erreurs
  <secondary>
    @type file
    path /fluentd/log/failed_records
    compress gzip
  </secondary>
</match>

# Sortie: Logs de tous les services (catch-all)
<match **>
  @type elasticsearch
  host elasticsearch
  port 9200
  logstash_format true
  logstash_prefix drupal-system-logs
  logstash_dateformat %Y.%m.%d
  include_tag_key true
  tag_key @log_name

  # Configuration du buffer en mode ASYNCHRONE
  <buffer>
    @type file
    path /fluentd/log/buffer/system
    flush_mode async
    flush_thread_count 2
    flush_interval 10s
    flush_at_shutdown true
    retry_type exponential_backoff
    retry_wait 2s
    retry_max_interval 60s
    retry_timeout 1h
    overflow_action block
    chunk_limit_size 5M
    queue_limit_length 16
    compress gzip
  </buffer>
</match>
